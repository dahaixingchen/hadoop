# hdfs的完全分布式部署

## 一、基础设施搭建

1. 安装zookeeper这个组件，如果不会的，可以联系我
2. 它的作用是实现NameNode 主备的自动切换

## 二、修改刚才上一篇文章提到的伪分布式部署

1. core-site.xml文件

   ```xml
   <property>
       <name>fs.defaultFS</name>
       <value>hdfs://mycluster</value>
   </property>
   
   <property>
       <name>ha.zookeeper.quorum</name>
       <value>node02:2181,node03:2181,node04:2181</value>
   </property>
   ```

   

2. hdfs-site.xml文件

   要先修改伪分布式模式下的两个地方

   ​	a. 把和secondaryNameNode相关的配置都删掉（Ha模式下没有secondaryNameNode）

   ​	b. 把NN相关的路径修改为 /var/bigdata/hadoop/ha/dfs/name，DN修改为/var/bigdata/hadoop/ha/dfs/data

   ```xml
   #以下是  一对多，逻辑到物理节点的映射
   <property>
       <name>dfs.nameservices</name>
       <value>mycluster</value>
   </property>
   <property>
       <name>dfs.ha.namenodes.mycluster</name>
       <value>nn1,nn2</value>
   </property>
   <property>
       <name>dfs.namenode.rpc-address.mycluster.nn1</name>
       <value>node01:8020</value>
   </property>
   <property>
       <name>dfs.namenode.rpc-address.mycluster.nn2</name>
       <value>node02:8020</value>
   </property>
   <property>
       <name>dfs.namenode.http-address.mycluster.nn1</name>
       <value>node01:50070</value>
   </property>
   <property>
       <name>dfs.namenode.http-address.mycluster.nn2</name>
       <value>node02:50070</value>
   </property>
   
   #以下是JN在哪里启动，数据存那个磁盘
   <property>
       <name>dfs.namenode.shared.edits.dir</name>
       <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value>
   </property>
   <property>
       <name>dfs.journalnode.edits.dir</name>
       <value>/var/bigdata/hadoop/ha/dfs/jn</value>
   </property>
   
   #HA角色切换的代理类和实现方法，我们用的ssh免密
   <property>
       <name>dfs.client.failover.proxy.provider.mycluster</name>
       <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
   </property>
   <property>
       <name>dfs.ha.fencing.methods</name>
       <value>sshfence</value>
   </property>
   <property>
       <name>dfs.ha.fencing.ssh.private-key-files</name>
       <value>/root/.ssh/id_dsa</value>
   </property>
   
   #开启自动化： 启动zkfc
   <property>
       <name>dfs.ha.automatic-failover.enabled</name>
       <value>true</value>
   </property>
   ```

   

3. 其他的，如 slaves 和 hadoop-env.sh文件不需要修改

## 三、启动集群

1. 这个一定要注意前后启动的顺序，下面我贴两张HDFS 

   ![img](doc/hadoop-image/Hdfs的ha方案.png)

   ![](doc/hadoop-image/ha模式下hdfs的官方启动流程.png)

2. 稍微解释下上面的图：hdfs是通过启动两个NN，一个主一个从的方式实现的

3. 主从之间通过journalNode来同步数据，所有这里我们要先启动三台机器的journalNode，命令：hdoop-daemon.sh  start journalnode

4. 由于是首次启动集群，所有要先格式化NN，所有选其中的一台进行格式化：   hdfs     namenode   -format 

5. 由于另一台机器需要同步数据，所有要先把刚才格式化的那个NN启动起来：hadoop-daemon.sh start namenode

6. 然后按照官方的启动流程，接下来就是要在另一台NN的机器上启动：hdfs namenode -bootstrapStandby

7. 接着格式化zk：   hdfs zkfc  -formatZK     <只有第一次搭建做，以后不用做>

8. 最后 start-dfs.sh	

## 四、使用验证

​	1）去看jn的日志和目录变化：

​			当name格式化后，它就会吧NN中的数据给同步过来

​	2）node04
​		zkCli.sh 
​			ls /
​			启动之后可以看到锁：
​			get  /hadoop-ha/mycluster/ActiveStandbyElectorLock

```shell
myclusternn1node01 �>(�>
cZxid = 0x360000003b
ctime = Sat Dec 12 19:47:46 CST 2020
mZxid = 0x360000003b
mtime = Sat Dec 12 19:47:46 CST 2020
pZxid = 0x360000003b
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x376563641fd0004
dataLength = 30
numChildren = 0

```

​	3）杀死namenode 杀死zkfc
​		kill -9  xxx
​		a)杀死active NN
​		b)杀死active NN身边的zkfc
​		c)shutdown activeNN 主机的网卡 ： ifconfig eth0 down
​			2节点一直阻塞降级
​			如果恢复1上的网卡   ifconfig eth0 up  
​			最终 2编程active

## 总结

如果在搭建的过程中出现问题，

要仔细看日志：日志的地址一般在：启动的时候会提醒你，具体的xxxx.out，你把out换成xxx.log，是直接换，