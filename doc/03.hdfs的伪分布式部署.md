

## 一、基础设施搭建

1. 设置网络，IP

2. 设置主机名和IP的映射关系

3. 关闭防火墙

4. 时钟同步

   1. 手动阀

5. 安装jdk

   上面的5步我假设你都会了，如果不会可以找我，我给你资料

6. ssh免密

   ```shell
   [root@node01 ~]# ssh localhost 
   	* 验证自己对自己有没有免密，
   	* 被动生成 ~/.ssh文件
   [root@node01 ~]# ssh-keygen -t dsa -p '' -f ~/.ssh/id_dsa
   	* -t dsa 表示用dsa的方式加密生成公钥秘钥
   	* -p 是加密时候的密码
   	* -f 表示把加密后的文件放在哪里
   [root@node01 ~]# cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
   	* 把公钥拷贝到授权钥匙的文件中
   ```

   如果A想要免密登陆B的话，就需要吧A 的公钥放到B 的authorized_keys 文件中

## 二、hdfs的配置（根据官网，应用的搭建过程）

1. 规划路径

   ```shell
   [root@node01 ~]# mkdir /opt/bigdata
   [root@node01 ~]# tar xf hadoop-2.6.5.tar.gz
   [root@node01 ~]# mv hadoop-2.6.5  /opt/bigdata/
   [root@node01 ~]# vi /etc/profile
   	* 加入如下的环境变量： export  JAVA_HOME=/usr/java/default
   		export HADOOP_HOME=/opt/bigdata/hadoop-2.6.5
   		export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   [root@node01 ~]# source /etc/profile
   ```

   

2. 配置hadoop的角色

   ```shell
   [root@node01 ~]# cd   $HADOOP_HOME/etc/hadoop
   	* 必须给hadoop配置javahome要不ssh过去找不到
   vi hadoop-env.sh
   	* export JAVA_HOME=/usr/java/default
   vi core-site.xml
   	* 给出NN角色在哪里启动
   	* <property>
       	<name>fs.defaultFS</name>
       	<value>hdfs://node01:9000</value>
      	 </property>
   vi hdfs-site.xml	
   	* 配置hdfs的副本数
   	* 		<property>
   				<name>dfs.replication</name>
   				<value>1</value>
   			</property>
   ```

   

3. 上面的这些配置都是可以在打开官网，找到老版本网站对应的描述（本人对老版本的网站的样式比较熟悉）

   ![hadoop](doc/hadoop-image/hadoop官网老版本点击.png)

   ![img](doc/hadoop-image/最简单的单机配置.png)

   

4. 其实到这步已经是可以吧hdfs给启动起来了，但是在启动的时候我们会发现secondaryNameNode会在本机启动，而且经过观察它的输出日志，会在0.0.0.0上，所有，我们需要配置secondaryNameNode相关的配置

   ```shell
   vi hdfs-site.xml
   	* <property>
   		<name>dfs.namenode.secondary.http-address</name>
   		<value>node02:50090</value>
   	  </property>
   	  
   	  <property>
         	<name>dfs.namenode.checkpoint.dir</name>
           <value>/var/bigdata/hadoop/full/dfs/secondary</value>
         </property>
         
   ```

5. 配置DataNode的位置

   ```sh
   vi slaves
   	* node02
   	  node03
   	  node04
   ```

   

6. 分发：

   ```shell
   scp -r ./bigdata/  node02:`pwd`
   scp -r ./bigdata/  node03:`pwd`
   scp -r ./bigdata/  node04:`pwd`
   ```

## 三、启动和初始化

1. hdfs namenode -format （这个是第一次启动集群的时候才运行）
2. start-dfs.sh（启动整个的hdfs集群）

## 四、简单使用

```shell
hdfs dfs -mkdir /bigdata
hdfs dfs -mkdir  -p  /user/root
```
## 五、验证知识点

```shell
cd   /var/bigdata/hadoop/local/dfs/name/current
	结合http//node01:50070的网页，观察 editlog的id是不是再fsimage的后边
cd /var/bigdata/hadoop/local/dfs/secondary/current
	结合http//node01:50070的网页，观察 SNN 只需要从NN拷贝最后时点的FSimage和增量的Editlog
	

hdfs dfs -put hadoop*.tar.gz  /user/root
cd  /var/bigdata/hadoop/local/dfs/data/current/BP-281147636-192.168.150.11-1560691854170/current/finalized/subdir0/subdir0
		

for i in `seq 100000`;do  echo "hello hadoop $i"  >>  data.txt  ;done
hdfs dfs -D dfs.blocksize=1048576  -put  data.txt 
cd  /var/bigdata/hadoop/local/dfs/data/current/BP-281147636-192.168.150.11-1560691854170/current/finalized/subdir0/subdir0
	检查data.txt被切割的块，他们数据什么样子
```
